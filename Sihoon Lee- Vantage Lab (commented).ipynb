{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages from PyTorch and Huggingface Transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM, AdamW\n",
    "import os\n",
    "\n",
    "# Disable annoying symlink warning from Huggingface Transformers\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "# Function that reads data and constructs a dataset\n",
    "def create_dataset(src_file, ref_file):\n",
    "    data = []\n",
    "    \n",
    "    # Read lines from files for source and reference sentences\n",
    "    with open(src_file) as src_file, open(ref_file) as ref_file:\n",
    "        for src, ref in zip(src_file, ref_file):\n",
    "            # Strip newlines and split the sentence into individual words (tokens)\n",
    "            src = src.rstrip()\n",
    "            ref = ref.rstrip()\n",
    "            src_tokens = src.split()\n",
    "            ref_tokens = ref.split()\n",
    "            \n",
    "            # Compare each token in source and reference sentences\n",
    "            for i, (src_token, ref_token) in enumerate(zip(src_tokens, ref_tokens)):\n",
    "                # If tokens are different, append to data\n",
    "                # src: source sentence\n",
    "                # i: index of different token\n",
    "                # ref_token: correct token from reference sentence\n",
    "                if src_token != ref_token:\n",
    "                    data.append((src, i, ref_token))\n",
    "    \n",
    "    # Returns list of tuples with source sentences and indexes of different tokens\n",
    "    return data\n",
    "\n",
    "\n",
    "class CorrectWordDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence, mask_index, correct_word = self.data[index]\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            max_length=128,     # Maximum length of the sentence\n",
    "            truncation=True,    # Truncate if longer than max_length\n",
    "            padding=\"max_length\",  # Pad if shorter than max_length\n",
    "            add_special_tokens=True,  # Add special tokens [CLS] and [SEP]\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'  # Return tensors\n",
    "        )\n",
    "\n",
    "        # Check whether the mask_index is outside of the max length\n",
    "        if mask_index + 1 >= 128:\n",
    "            return None\n",
    "\n",
    "        # Squeeze redundant dimensions\n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        mask_token_id = self.tokenizer.mask_token_id\n",
    "        correct_token_id = self.tokenizer.encode(\n",
    "            correct_word, add_special_tokens=False)[0]\n",
    "\n",
    "        # Replace the token at the mask index with the mask token\n",
    "        input_ids[mask_index + 1] = mask_token_id\n",
    "        # Create a labels tensor with -100 everywhere other than the mask index\n",
    "        labels = torch.full_like(input_ids, -100)\n",
    "        labels[input_ids==mask_token_id] = correct_token_id\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Remove None values from the batch\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    # Pad the input_ids, attention_mask and labels to the same length\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "def predict(model, tokenizer, dataloader, test_data, device):\n",
    "    # Put the model in eval mode\n",
    "    model.eval()\n",
    "    index = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Get the output from the model\n",
    "        output_dict = model(input_ids, attention_mask=attention_mask)\n",
    "        # Get the predictions from the output logits\n",
    "        predictions = output_dict.logits.argmax(dim=-1)\n",
    "        \n",
    "        # Get the original sentence, mask index, and correct word from the test data\n",
    "        sentence, mask_index, _ = test_data[index]\n",
    "        # Get the predicted word id\n",
    "        predicted_word_id = predictions[0, mask_index + 1]\n",
    "        # Convert the predicted word id to the word\n",
    "        predicted_word = tokenizer.convert_ids_to_tokens([predicted_word_id])[0]\n",
    "\n",
    "        # Print the original sentence and the proposed correct word\n",
    "        print(f'Original sentence: {sentence.strip()}')\n",
    "        print(f'Proposed correct word: {predicted_word}')\n",
    "        \n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: did\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: did\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: did\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: did\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: did\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: did\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: did\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: did\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: did\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: did\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load the DistilBert tokenizer and model from the Hugging Face model hub\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    # Set the path for the source and reference files\n",
    "    src_file = 'C:/Users/beckh/Grammar Checking/jfleg-master/jfleg-master/dev/dev.src'\n",
    "    ref_file = 'C:/Users/beckh/Grammar Checking/jfleg-master/jfleg-master/dev/dev.ref0'\n",
    "    # Create the dataset from the source and reference files\n",
    "    data = create_dataset(src_file, ref_file)\n",
    "    # Use a small subset of data\n",
    "    data = data[:10]\n",
    "    \n",
    "    # Create the CorrectWordDataset object from the data and tokenizer\n",
    "    dataset = CorrectWordDataset(data, tokenizer)\n",
    "    # Create the data loader from the dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Add this\n",
    "    # Define the optimizer for the model\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-1)\n",
    "    \n",
    "    # Check if CUDA is available and set the device accordingly\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Transfer the model to the device\n",
    "    model.to(device)\n",
    "\n",
    " # Reduce the number of epochs from 3 to 1\n",
    "    num_epochs = 1\n",
    "    for epoch in range(1):\n",
    "        # Iterate over batches in the data loader\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            # Extract input_ids, attention_mask, and labels from the batch, send to the device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Zero the optimizer gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass the inputs through the model\n",
    "            output = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            # Extract the loss from the output\n",
    "            loss = output.loss\n",
    "            # Compute backward gradients\n",
    "            loss.backward()\n",
    "            # Update optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "    # Load test data\n",
    "    test_src_file = 'C:/Users/beckh/Grammar Checking/jfleg-master/jfleg-master/test/test.src'\n",
    "    test_ref_file = 'C:/Users/beckh/Grammar Checking/jfleg-master/jfleg-master/test/test.ref0'\n",
    "    test_data = create_dataset(test_src_file, test_ref_file)\n",
    "    test_data = test_data[:10]\n",
    "    test_dataset = CorrectWordDataset(test_data, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    predict(model, tokenizer, test_dataloader, test_data, device)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_pretrained('C:/Users/beckh/Grammar Checking/jfleg')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: and\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: and\n",
      "Original word: new\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: improved\n",
      "Original word: technology\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: technology\n",
      "Original word: has\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: have\n",
      "Original word: been\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: been\n",
      "Original word: introduced\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: introduced\n",
      "Original word: to\n",
      "Original sentence: New and new technology has been introduced to the society .\n",
      "Proposed correct word: into\n",
      "Original word: the\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: ##ization\n",
      "Original word: richer\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: levels\n",
      "Original word: countries\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: in\n",
      "Original word: will\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: the\n",
      "Original word: outweigh\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: poorer\n",
      "Original word: any\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: countries\n",
      "Original word: rise\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: will\n",
      "Original word: in\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: out\n",
      "Original word: motorization\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: ##weig\n",
      "Original word: levels\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: ##h\n",
      "Original word: in\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: a\n",
      "Original word: the\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: reduction\n",
      "Original word: poorer\n",
      "Original sentence: One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
      "Proposed correct word: in\n",
      "Original word: sciences\n",
      "Original sentence: Every person needs to know a bit about math , sciences , arts , literature and history in order to stand out in society .\n",
      "Proposed correct word: science\n",
      "Original word: arts\n",
      "Original sentence: Every person needs to know a bit about math , sciences , arts , literature and history in order to stand out in society .\n",
      "Proposed correct word: mathematics\n",
      "Original word: ,\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: highly\n",
      "Original word: that\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: unlikely\n",
      "Original word: the\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: enough\n",
      "Original word: company\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: because\n",
      "Original word: will\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: the\n",
      "Original word: tell\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: customer\n",
      "Original word: about\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: will\n",
      "Original word: the\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: complain\n",
      "Original word: sites\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: them\n",
      "Original word: that\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: specific\n",
      "Original word: were\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: sites\n",
      "Original word: not\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: that\n",
      "Original word: included\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: are\n",
      "Original word: in\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: previously\n",
      "Original word: the\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: included\n",
      "Original word: tour\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: on\n",
      "Original word: --\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: the\n",
      "Original word: for\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: package\n",
      "Original word: example\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: package\n",
      "Original word: due\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: price\n",
      "Original word: to\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: for\n",
      "Original word: entrance\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: example\n",
      "Original word: fees\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: due\n",
      "Original word: that\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: to\n",
      "Original word: would\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: travel\n",
      "Original word: make\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: fees\n",
      "Original word: the\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: which\n",
      "Original word: total\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: would\n",
      "Original word: package\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: make\n",
      "Original word: price\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: the\n",
      "Original word: overly\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: ticket\n",
      "Original word: expensive\n",
      "Original sentence: While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
      "Proposed correct word: ticket\n",
      "Original word: Disadvantage\n",
      "Original sentence: Disadvantage is parking their car is very difficult .\n",
      "Proposed correct word: whoever\n",
      "Original word: is\n",
      "Original sentence: Disadvantage is parking their car is very difficult .\n",
      "Proposed correct word: when\n",
      "Original word: parking\n",
      "Original sentence: Disadvantage is parking their car is very difficult .\n",
      "Proposed correct word: that\n",
      "Original word: their\n",
      "Original sentence: Disadvantage is parking their car is very difficult .\n",
      "Proposed correct word: by\n",
      "Original word: car\n",
      "Original sentence: Disadvantage is parking their car is very difficult .\n",
      "Proposed correct word: vehicles\n",
      "Original word: is\n",
      "Original sentence: Disadvantage is parking their car is very difficult .\n",
      "Proposed correct word: is\n",
      "Original word: very\n",
      "Original sentence: Disadvantage is parking their car is very difficult .\n",
      "Proposed correct word: extremely\n",
      "Original word: difficult\n",
      "Original sentence: Disadvantage is parking their car is very difficult .\n",
      "Proposed correct word: expensive\n",
      "Original word: .\n",
      "Original sentence: Disadvantage is parking their car is very difficult .\n",
      "Proposed correct word: .\n",
      "Original word: are\n",
      "Original sentence: Bigger farming are use more chemical product and substance to feed fish .\n",
      "Proposed correct word: methods\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 64 is out of bounds for dimension 0 with size 64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-55621e9cc920>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mmain_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-55621e9cc920>\u001b[0m in \u001b[0;36mmain_pretrained\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mpredicted_word_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mpredicted_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredicted_word_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 64 is out of bounds for dimension 0 with size 64"
     ]
    }
   ],
   "source": [
    "def main_pretrained():\n",
    "    # Load pre-trained DistilBERT model and tokenizer\n",
    "    model_path = 'distilbert-base-uncased'\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "    model = DistilBertForMaskedLM.from_pretrained(model_path)\n",
    "\n",
    "    # Load test data\n",
    "    test_src_file = 'C:/Users/beckh/Grammar Checking/jfleg-master/jfleg-master/test/test.src'\n",
    "    test_ref_file = 'C:/Users/beckh/Grammar Checking/jfleg-master/jfleg-master/test/test.ref0'\n",
    "    test_data = create_dataset(test_src_file, test_ref_file)\n",
    "    \n",
    "    # Create DataLoader from test data\n",
    "    test_dataset = CorrectWordDataset(test_data, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Send model to GPU or CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Loop over test dataloader\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass of the model\n",
    "        output_dict = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = output_dict.logits.argmax(dim=-1)\n",
    "\n",
    "        # Loop over the range of the length of the test dataset\n",
    "        for index in range(len(test_data)):\n",
    "            sentence, mask_index, _ = test_data[index]\n",
    "\n",
    "            # Retrieve predicted token index\n",
    "            predicted_word_id = predictions[index, mask_index + 1]\n",
    "            \n",
    "            # Decode the index to the corresponding word\n",
    "            predicted_word = tokenizer.decode([predicted_word_id])\n",
    "\n",
    "            # Print the original word, original sentence, and the proposed correct word\n",
    "            print(f'Original word: {sentence.split()[mask_index]}')\n",
    "            print(f'Original sentence: {sentence.strip()}')\n",
    "            print(f'Proposed correct word: {predicted_word}')\n",
    "            # This increment of 'index' is redundant because it is already incremented by the for loop\n",
    "            index += 1\n",
    "if __name__ == '__main__':\n",
    "    main_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: There knowledge of those facts was incomplete!\n",
      "Proposed correct word: ['did', 'if', 'develop', 'little', 'battle']\n",
      "Original sentence: Their going to learn something new from the ML course.\n",
      "Proposed correct word: ['did', 'if', 'develop', 'little', 'battle']\n"
     ]
    }
   ],
   "source": [
    "def predict_on_sentence(model, tokenizer, src_sentence, device):\n",
    "    src_sentence = str(src_sentence)\n",
    "    data = []\n",
    "    src_words = src_sentence.split() # Split the source sentence into individual words\n",
    "    \n",
    "    # Use a for loop to create data, with index and words in src_words using enumerate\n",
    "    for mask_index, src_word in enumerate(src_words):\n",
    "        data.append((src_sentence, mask_index, None))\n",
    "    \n",
    "    # Create a dataset using CorrectWordDataset with the data and tokenizer\n",
    "    dataset = CorrectWordDataset(data, tokenizer)\n",
    "    # Create a dataloader using DataLoader with the created dataset and other parameters\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model.to(device) # Moves and/or casts the model parameters and buffers\n",
    "    result = []\n",
    "    index = 0\n",
    "    for batch in dataloader: # For each batch in the dataloader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        output_dict = model(input_ids, attention_mask=attention_mask) # Pass input_ids and attention_mask to model\n",
    "        predictions = output_dict.logits.argmax(dim=-1) # Get the predicted words from the output_dict logits\n",
    "\n",
    "        for _, mask_index, _ in data[index:index+len(predictions)]:\n",
    "            predicted_word_id = predictions[index % 64, mask_index + 1] # predicted_word_id is equal to the predicted word in the predictions\n",
    "            predicted_word = tokenizer.decode([predicted_word_id]) # Decode the predicted_word from predicted_word_id\n",
    "            \n",
    "            result.append(predicted_word.strip()) # Append the predicted word to the result list\n",
    "\n",
    "            index += 1\n",
    "\n",
    "    return ' '.join(result) # Return the result list as a single sentence\n",
    "\n",
    "def main():\n",
    "    # Load the pretrained model\n",
    "    model_dir = \"C:/Users/beckh/Grammar Checking/jfleg\"  # replace with your own path\n",
    "    model = DistilBertForMaskedLM.from_pretrained(model_dir)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "    # Testing sentences\n",
    "    sentences = [\n",
    "        \"There knowledge of those facts was incomplete!\",\n",
    "        \"Their going to learn something new from the ML course.\"\n",
    "    ]\n",
    "\n",
    "    # Perform inference (i.e., making predictions) on the testing sentences\n",
    "    predict_on_sentences(model, tokenizer, sentences)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, tokenizer, dataloader, test_data, device):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    index = 0\n",
    "    correct = 0 # Counter for the number of correct predictions\n",
    "\n",
    "    for batch in dataloader: # For each batch in the dataloader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        output_dict = model(input_ids, attention_mask=attention_mask) # Pass input_ids and attention_mask to model\n",
    "        predictions = output_dict.logits.argmax(dim=-1) # Get the predicted words from the output_dict logits\n",
    "\n",
    "        # For each item in the test_data within the batch\n",
    "        for _, mask_index, ref_word in test_data[index:index+len(predictions)]:\n",
    "            predicted_word_id = predictions[index % 64, mask_index + 1] # Get the predicted word id at the corresponding index\n",
    "            predicted_word = tokenizer.decode([predicted_word_id]).strip() # Decode the predicted_word from predicted_word_id\n",
    "\n",
    "            if predicted_word == ref_word: # If predicted_word matches ref_word\n",
    "                correct += 1 # Increment the number of correct predictions\n",
    "\n",
    "            index += 1 # Increment the index to move to next item in batch\n",
    "\n",
    "    return correct / len(test_data) # Calculate accuracy as ratio of correct predictions to total number of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-ab11119e8e22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Accuracy: {accuracy}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-b466d0671590>\u001b[0m in \u001b[0;36mcalculate_accuracy\u001b[1;34m(model, tokenizer, dataloader, test_data, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mpredicted_word_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mpredicted_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredicted_word_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"len() of a 0-d tensor\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m             warnings.warn('Using len to get tensor shape might cause the trace to be incorrect. '\n\u001b[0;32m    592\u001b[0m                           \u001b[1;34m'Recommended usage would be tensor.shape[0]. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_src_file = 'C:/Users/beckh/Grammar Checking/jfleg-master/jfleg-master/test/test.src'\n",
    "test_ref_file = 'C:/Users/beckh/Grammar Checking/jfleg-master/jfleg-master/test/test.ref0'\n",
    "test_data = create_dataset(test_src_file, test_ref_file)\n",
    "test_dataset = CorrectWordDataset(test_data, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "accuracy = calculate_accuracy(model, tokenizer, test_dataloader, test_data, device)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
